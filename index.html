<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0035)https://weiyang-hust.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="“width=800”">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

a {
color: #1772d0;
text-decoration: none;
}

a:focus,
a:hover {
color: #f09228;
text-decoration: none;
}

body,
td,
th,
tr,
p,
a {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px
}

strong {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
}

heading {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 22px;
}

papertitle {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
font-weight: 700
}

name {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 32px;
font-weight: bold;
}

affliation {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 14px;  
  font-style: oblique;
}

.one {
width: 160px;
height: auto;
position: relative;
}

.one_vc {
  margin: 0;
  position: relative;
  top: 50%;
  -ms-transform: translateY(-50%);
  transform: translateY(-50%);
}

.two {
width: 160px;
height: 160px;
position: absolute;
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

.two_vc {
  width: 160px;
  height: 160px;
  transition: opacity .2s ease-in-out;
  -moz-transition: opacity .2s ease-in-out;
  -webkit-transition: opacity .2s ease-in-out;
  position: absolute;
  top: 50%;
  -ms-transform: translateY(-50%);
  transform: translateY(-50%);
  }
  

.fade {
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

span.highlight {
background-color: #ffffd0;
}
  </style>
  <link rel="icon" type="image/png" href="./img/logo.png">
  <title>Wei Yang</title>
  
  <link href="./wei_yang_files/css" rel="stylesheet" type="text/css">

  </head>
  <body data-new-gr-c-s-check-loaded="14.1101.0" data-gr-ext-installed="">
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="left">
          <name>Wei Yang (杨卫)</name>
        </p>
        <p align="left">
          <affliation> Associate Professor<br>
            School of Computer Science & Technology, HUST<br>
            Room 306, Southern Building #6            
            </affliation>
          </p>
          <p>
            I am an Associate Professor at the <a href="http://english.cs.hust.edu.cn/">School of Computer Science, Huazhong University of Science and Technology</a>, where I co-lead the <a href="http://media.hust.edu.cn/">HUST Media Lab</a>. My research interests primarily lie in the areas of imaging, graphics, computer vision, and artificial intelligence.
        </p>
        <p>
            Before joining HUST, I worked in the <a href="https://atap.google.com/">Advanced Technology and Projects (ATAP)</a> division at <a href="https://about.google/">Google</a> in Mountain View, USA. As a proud member of the Te'veren team, I collaborated with Rick Marks on advanced sensing and on-device intelligence using computer vision. Prior to Google, I served as a Principal Scientist at <a href="https://www.us1.dgene.com/">DGene, US</a>, where I conducted research on real-time volumetric human capture systems.
        </p>
        <p>
            I graduated from the <a href="https://udel.edu">University of Delaware</a> in 2017, where I majored in Computer Science. At UDel, I worked with Professor <a href="http://www.yu-jingyi.com/">Jingyi Yu</a> on research problems in computational photography and scene understanding. During my PhD, I interned at Adobe, hosted by the ACR team, in 2015.
        </p>
        <p>
          <font color="red"><strong>I am actively seeking creative and highly motivated MS and PhD students who are passionate about research.</strong></font>
        </p>
        <p align="center">
          <a href="mailto:weiyangcs@hust.edu.cn">Email</a> &nbsp;/&nbsp;
	      <a href="https://scholar.google.com/citations?user=fRjxdPgAAAAJ&amp;hl">Google Scholar</a> &nbsp;
        </p>
        </td>
        <td width="33%">
	  <a href="https://weiyang-hust.github.io/img/avatar_narrow.png">
        <img src="./img/avatar_narrow.png" width="250px"></a>
        </td>
      </tr>
      </tbody></table>
    
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td width="100%" valign="middle">
            <heading>Services</heading>
            <ul><li><strong>Area Chair</strong>: CVPR 2023</li></ul>
            <ul><li><strong>Program Committee</strong>: AAAI 20, AAAI 21, WACV 21, BMVC 18</li></ul>
            <ul><li><strong>Reviewer</strong>: CVPR, ICCV, NeurIPS, ICML, ECCV, TPAMI, TIP, TVCG...</li></ul>
          </td>
        </tr>
        </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
          <heading>Research and Publications</heading>
          <p>
            * denotes equal contribution or co-corresponding author
          </p>
        </td>
      </tr>
      </tbody></table>
  
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tbody>
    <!-- Publication Section: ADP -->
    <tr onmouseout="adp_stop()" onmouseover="adp_start()">
      <td valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="adp_image" style="opacity: 0;"><img src="./pub/adp.gif" width="160"></div>
          <img src="./pub/adp.png" width="160">
        </div>
        <script type="text/javascript">
          function adp_start() {
            document.getElementById('adp_image').style.opacity = "1";
          }

          function adp_stop() {
            document.getElementById('adp_image').style.opacity = "0";
          }
          adp_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://openreview.net/forum?id=sc3sKQqszx">
          <papertitle>Adaptive Patch Deformation for Textureless-Resilient Multi-View Stereo</papertitle>
        </a>
        <br>
        Yuesong Wang, 
        Zhaojie Zeng,
        Tao Guan,
        <strong>Wei Yang</strong>,
        Zhuo Chen,
        Wenkai Liu,
        Luoyuan Xu,
        <a href="https://github.com/RoyalVane">Yawei Luo</a>
        <br>
        <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2023 &nbsp;
        <br>
        arXiv /
        video /
        Code (Coming Soon)
        <p></p>
        <p>We transplant the spirit of deformable convolution into the PatchMatch-based method for both memory-friendly and textureless-resilient MVS.</p>
      </td>
    </tr>

    <!-- Publication Section: CTT -->
    <tr onmouseout="cttrack_stop()" onmouseover="cttrack_start()">
      <td valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="cttrack_image" style="opacity: 0;"><img src="./pub/cttrack.png" width="160"></div>
          <img src="./pub/cttrack.png" width="160">
        </div>
        <script type="text/javascript">
          function cttrack_start() {
            document.getElementById('cttrack_image').style.opacity = "1";
          }

          function cttrack_stop() {
            document.getElementById('cttrack_image').style.opacity = "0";
          }
          cttrack_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2301.10938">
          <papertitle>Compact Transformer Tracker with Correlative Masked Modeling</papertitle>
        </a>
        <br>
        <a href="https://github.com/SkyeSong38">Zikai Song</a>, 
        Run Luo,
        Junqing Yu,
        <a href="http://homepage.cs.latrobe.edu.au/ypchen/">Yi-Ping Phoebe Chen</a>,
        <strong>Wei Yang</strong>,
        <br>
        <em>AAAI</em>, 2023 &nbsp;<font color="tomato"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://arxiv.org/abs/2301.10938">arXiv</a>
        /
        <a href="https://github.com/HUSTDML/CTTrack">Code</a>
        <p></p>
        <p>We demonstrate the basic vision transformer (ViT) architecture is sufficient for visual tracking with correlative masked modeling for information aggregation enhancement.</p>
      </td>
    </tr>

    <!-- Publication Section: DMU -->
    <tr onmouseout="dmu_stop()" onmouseover="dmu_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="dmu_image" style="opacity: 0;"><img src="./pub/dmu.png" width="140"></div>
          <img src="./pub/dmu.png" width="140">
        </div>
        <script type="text/javascript">
          function dmu_start() {
            document.getElementById('dmu_image').style.opacity = "1";
          }

          function dmu_stop() {
            document.getElementById('dmu_image').style.opacity = "0";
          }
          dmu_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2302.05160">
          <papertitle>Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection</papertitle>
        </a>
        <br>
        <a href="https://github.com/henrryzh1">Hang Zhou</a>, 
        Junqing Yu,
        <strong>Wei Yang</strong>,
        <br>
        <em>AAAI</em>, 2023 &nbsp;<font color="tomato"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://arxiv.org/abs/2302.05160">arXiv</a>
        /
        <a href="https://github.com/henrryzh1/UR-DMU">Code</a>
        <p></p>
        <p>We propose an Uncertainty Regulated Dual Memory Units (UR-DMU) model to learn both the representations of normal data and discriminative features of abnormal data.</p>
      </td>
    </tr>

    <!-- Publication Section: HumanNeRF -->
    <tr onmouseout="humannerf_stop()" onmouseover="humannerf_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="humannerf_image" style="opacity: 0;"><video width="85%" height="85%" muted="" autoplay="" loop="">
            <source src="./pub/humannerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="humannerf_static_image" src="./pub/humannerf.png" width="160">
        </div>
        <script type="text/javascript">
          function humannerf_start() {
            document.getElementById('humannerf_image').style.opacity = "1";
            document.getElementById('humannerf_static_image').style.opacity = "0";
          }

          function humannerf_stop() {
            document.getElementById('humannerf_static_image').style.opacity = "1";
            document.getElementById('humannerf_image').style.opacity = "0";
          }
          humannerf_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/pdf/2112.02789.pdf">
          <papertitle>HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs</papertitle>
        </a>
        <br>
        <a href="https://zhaofuq.github.io/">Fuqiang Zhao</a>, 
        <strong>Wei Yang</strong>,
        <a href="https://jiakai-zhang.github.io/">Jiakai Zhang</a>,
        Pei Lin,
        Yingliang Zhang, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>, 
        <a href="http://xu-lan.com/">Lan Xu</a>, 
        <br>
        <em>CVPR</em>, 2022 &nbsp;
        <br>
        <a href="https://zhaofuq.github.io/humannerf/">Project Page</a>
        /        
        <a href="https://arxiv.org/pdf/2112.02789.pdf">arXiv</a>
        /
        <a href="https://github.com/zhaofuq/HumanNeRF">Code</a>
        /
        <a href="https://www.youtube.com/watch?v=CKeXnQiLCd4&ab_channel=ShanghaiTechDigitalHuman">Video</a>
        <p></p>
        <p>We present a neural representation with efficient generalization ability for high-fidelity free-view synthesis of dynamic humans.</p>
      </td>
    </tr>

    <!-- Publication Section: CSWinTT -->
    <tr onmouseout="cswintt_stop()" onmouseover="cswintt_start()">
          <td valign="middle"  width="25%">
            <div class="one">
              <div class="two" id="cswintt_image" style="opacity: 0;"><img src="./pub/cswintt.png" width="160"></div>
              <img src="./pub/cswintt.png" width="160">
            </div>
            <script type="text/javascript">
              function cswintt_start() {
                document.getElementById('cttrack_image').style.opacity = "1";
              }
    
              function cswintt_stop() {
                document.getElementById('cttrack_image').style.opacity = "0";
              }
              cswintt_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2205.03806">
              <papertitle>Transformer Tracking With Cyclic Shifting Window Attention</papertitle>
            </a>
            <br>
            <a href="https://github.com/SkyeSong38">Zikai Song</a>, 
            Junqing Yu,
            <a href="http://homepage.cs.latrobe.edu.au/ypchen/">Yi-Ping Phoebe Chen</a>,
            <strong>Wei Yang</strong>,
            <br>
            <em>CVPR</em>, 2022 &nbsp;
            <br>
            <a href="https://arxiv.org/abs/2205.03806">arXiv</a>
            /
            <a href="https://github.com/SkyeSong38/CSWinTT">Code</a>
            <p></p>
            <p>CSWinTT is a new transformer architecture with multi-scale cyclic shifting window attention for visual object tracking, elevating the attention from pixel to window level.</p>
          </td>
        </tr>


    <!-- Publication Section: Artemis -->
    <tr onmouseout="artemis_stop()" onmouseover="artemis_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="artemis_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/artemis.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="artemis_static_image" src="./pub/artemis.png" width="160">
        </div>
        <script type="text/javascript">
          function artemis_start() {
            document.getElementById('artemis_static_image').style.opacity = "0";
            document.getElementById('artemis_image').style.opacity = "1";
          }

          function artemis_stop() {
            document.getElementById('artemis_image').style.opacity = "0";
            document.getElementById('artemis_static_image').style.opacity = "1";
          }
          artemis_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://haiminluo.github.io/publication/artemis/">
          <papertitle>Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</papertitle>
        </a>
        <br>
        <a href="https://haiminluo.github.io/">Haimin Luo</a>, 
        Teng Xu, 
        <a href="https://nowheretrix.github.io/">Yuheng Jiang</a>, 
        Chenglin Zhou, 
        Qiwei Qiu, 
        Yingliang Zhang, 
        <strong>Wei Yang</strong>,
        <a href="http://xu-lan.com/">Lan Xu</a>, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
        <br>
        <em>SIGGRAPH</em>, 2022 &nbsp;
        <br>
        <a href="https://haiminluo.github.io/publication/artemis/">Project Page</a>
        /        
        <a href="https://arxiv.org/abs/2202.05628">arXiv</a>
        /
        <a href="https://github.com/HaiminLuo/Artemis">Code</a>
        /
        <a href="https://www.youtube.com/watch?v=Asbt3BYcZMM&ab_channel=ShanghaiTechDigitalHuman">Video</a>
        <p></p>
        <p>ARTEMIS, the core of which is a neural-generated (NGI) animal engine, enables interactive motion control, real-time animation and photo-realistic rendering of furry animals.</p>
      </td>
    </tr>

    <!-- Publication Section: NeuVV -->
    <tr onmouseout="neuvv_stop()" onmouseover="neuvv_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="neuvv_image" style="opacity: 0;"><img src="./pub/neuvv.png" width="160"></div>
          <img id="neuvv_static_image" src="./pub/neuvv.png" width="160">
        </div>
        <script type="text/javascript">
          function neuvv_start() {
            document.getElementById('neuvv_image').style.opacity = "1";
          }

          function neuvv_stop() {
            document.getElementById('neuvv_image').style.opacity = "0";
          }
          neuvv_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://haiminluo.github.io/publication/artemis/">
          <papertitle>NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing</papertitle>
        </a>
        <br>
        <a href="https://jiakai-zhang.github.io/">Jiakai Zhang</a>,
        <a href="https://aoliao12138.github.io/">Liao Wang</a>, 
        <a href="http://xinhangliu.com/">Xinhang Liu</a>, 
        <a href="https://zhaofuq.github.io/">Fuqiang Zhao</a>, 
        Minzhang Li, 
        Haizhao Dai, 
        Boyuan Zhang, 
        <strong>Wei Yang</strong>,
        <a href="http://xu-lan.com/">Lan Xu</a>, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
        <br>
        <em>arXiv preprint</em>, 2202.06088 &nbsp;
        <br>   
        <a href="https://arxiv.org/pdf/2202.06088.pdf">arXiv</a>
        <p></p>
        <p>NeuVV introduces a hyper-spherical harmonics (HH) decomposition for modeling smooth color variations over space and time.</p>
      </td>
    </tr>
    <!-- Publication Section: ACG-CVCNet -->
    <tr onmouseout="acgcvc_stop()" onmouseover="acgcvc_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="acgcvc_image" style="opacity: 0;"><img src="./pub/acgcvc.png" width="160"></div>
          <img id="acgcvc_static_image" src="./pub/acgcvc.png" width="160">
        </div>
        <script type="text/javascript">
          function acgcvc_start() {
            document.getElementById('acgcvc_image').style.opacity = "1";
          }

          function acgcvc_stop() {
            document.getElementById('acgcvc_image').style.opacity = "0";
          }
          acgcvc_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3503161.3547926?casa_token=lbDaNtizQeUAAAAA:EGQEf4cDrdfqUY_uAckORK0Gny8NEUrNfFCTYMCPgG-cWfD_8mskpgb_FuJJQ__0_Nc9rLpvJJlgEQ">
          <papertitle>Self-Supervised Multi-view Stereo via Adjacent Geometry Guided Volume Completion</papertitle>
        </a>
        <br>
        Luoyuan Xu, 
        Tao Guan, 
        Yuesong Wang, 
        <a href="https://github.com/RoyalVane">Yawei Luo</a>
        Zhuo Chen, 
        Wenkai Liu, 
        <strong>Wei Yang</strong>
        <br> 
        <a href="https://dl.acm.org/doi/pdf/10.1145/3503161.3547926?casa_token=lbDaNtizQeUAAAAA:EGQEf4cDrdfqUY_uAckORK0Gny8NEUrNfFCTYMCPgG-cWfD_8mskpgb_FuJJQ__0_Nc9rLpvJJlgEQ">Paper</a>
        <p></p>
        <p>We propose the AGG-CVCNet to learn complete geometry inference from partial observations with high confidence.</p>
      </td>
    </tr>

    <!-- Publication Section: NPFA -->
    <tr onmouseout="npfa_stop()" onmouseover="npfa_start()">
          <td align="center" valign="middle"  width="25%">
            <div class="one">
              <div class="two_vc" id="npfa_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
                <source src="./pub/npfa.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              <img id="npfa_static_image" src="./pub/npfa.png" width="160">
            </div>
            <script type="text/javascript">
              function npfa_start() {
                document.getElementById('npfa_static_image').style.opacity = "0";
                document.getElementById('npfa_image').style.opacity = "1";
              }
    
              function npfa_stop() {
                document.getElementById('npfa_image').style.opacity = "0";
                document.getElementById('npfa_static_image').style.opacity = "1";
              }
              npfa_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2202.05592">
              <papertitle>Video-driven Neural Physically-based Facial Asset for Production</papertitle>
            </a>
            <br>
            <a href="https://zhanglongwen.com/">Longwen Zhang</a>, 
            Chuxiao Zeng, 
            Qixuan Zhang, 
            Hongyang Lin, 
            <a href="https://www.caoruixiang.com/resume/Resume.pdf">Ruixiang Cao</a>, 
            <strong>Wei Yang</strong>,
            <a href="http://xu-lan.com/">Lan Xu</a>, 
            <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
            <br>
            <em>SIGGRAPH Asia</em>, 2022 &nbsp;
            <br>
            <a href="https://sites.google.com/view/npfa/">Project Page</a>
            /
            <a href="https://arxiv.org/abs/2202.05592">arXiv</a>
            /
            <a href="https://github.com/HaiminLuo/Artemis">Code</a>
            /
            <a href="https://www.youtube.com/watch?v=BifBssghYWQ&ab_channel=ShanghaiTechDigitalHuman">Video</a>
            <p></p>
            <p>We present a learning-based, video-driven approach for generating dynamic facial geometries with high-quality physically-based assets</p>
          </td>
    </tr>

    <!-- Publication Section: Tightcap -->
    <tr onmouseout="tightcap_stop()" onmouseover="tightcap_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="tightcap_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/tightcap.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="tightcap_static_image" src="./pub/tightcap.jpg" width="160">
        </div>
        <script type="text/javascript">
          function tightcap_start() {
            document.getElementById('tightcap_static_image').style.opacity = "0";
            document.getElementById('tightcap_image').style.opacity = "1";
          }

          function tightcap_stop() {
            document.getElementById('tightcap_image').style.opacity = "0";
            document.getElementById('tightcap_static_image').style.opacity = "1";
          }
          tightcap_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://github.com/ChenFengYe/TightCap">
          <papertitle>Tightcap: 3D human shape capture with clothing tightness field</papertitle>
        </a>
        <br>
        <a href="https://chenxin.tech/">Xin Chen</a>, 
        Anqi Pang, 
        <strong>Wei Yang</strong>,
        <a href="https://peihaowang.github.io/">Peihao Wang</a>,
        <a href="http://xu-lan.com/">Lan Xu</a>, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
        <br>
        <em>SIGGRAPH</em>, 2022 (TOG)&nbsp;
        <br>
        <a href="https://chenxin.tech/TightCap/">Project Page</a>
        /
        <a href="https://arxiv.org/pdf/1904.02601.pdf">arXiv</a>
        /
        <a href="https://github.com/ChenFengYe/TightCap">Code</a>
        /
        <a href="https://www.youtube.com/watch?v=2_Vlq34s80Y&ab_channel=XinChen">Video</a>
        <p></p>
        <p>TightCap is a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan.</p>
      </td>
    </tr>

    <!-- Publication Section: SportsCap -->
    <tr onmouseout="sportscap_stop()" onmouseover="sportscap_start()">
          <td align="center" valign="middle"  width="25%">
            <div class="one">
              <div class="two_vc" id="sportscap_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
                <source src="./pub/sportscap.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              <img id="sportscap_static_image" src="./pub/sportscap.png" width="160">
            </div>
            <script type="text/javascript">
              function sportscap_start() {
                document.getElementById('sportscap_static_image').style.opacity = "0";
                document.getElementById('sportscap_image').style.opacity = "1";
              }
    
              function sportscap_stop() {
                document.getElementById('sportscap_image').style.opacity = "0";
                document.getElementById('sportscap_static_image').style.opacity = "1";
              }
              sportscap_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://chenxin.tech/files/Paper/IJCV2020_Sport/project_page_SportsCap/index.htm">
              <papertitle>SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</papertitle>
            </a>
            <br>
            <a href="https://chenxin.tech/">Xin Chen</a>, 
            Anqi Pang, 
            <strong>Wei Yang</strong>,
            <a href="https://yuexinma.me/">Yuexin Ma</a>, 
            <a href="http://xu-lan.com/">Lan Xu</a>, 
            <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>,
            <br>
            <em>IJCV</em>, 2022 (TOG)&nbsp;
            <br>
            <a href="https://chenxin.tech/files/Paper/IJCV2020_Sport/project_page_SportsCap/index.htm">Project Page</a>
            /
            <a href="https://arxiv.org/abs/2104.11452">arXiv</a>
            /
            <a href="https://github.com/ChenFengYe/SportsCap">Code</a>
            /
            <a href="https://www.youtube.com/watch?v=TrqAaaX97KY&ab_channel=ShanghaiTechDigitalHuman">Video</a>
            <p></p>
            <p>SportsCap -- the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input</p>
          </td>
      </tr>

    <!-- Publication Section: XSfM -->
    <tr onmouseout="xsfm_stop()" onmouseover="xsfm_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="xsfm_image" style="opacity: 0;"><img src="./pub/xsfm.png" width="160"></div>
          <img id="xsfm_static_image" src="./pub/xsfm.png" width="160">
        </div>
        <script type="text/javascript">
          function xsfm_start() {
            document.getElementById('xsfm_image').style.opacity = "1";
          }

          function xsfm_stop() {
            document.getElementById('xsfm_image').style.opacity = "0";
          }
          xsfm_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8918483">
          <papertitle>Structure From Motion on XSlit Cameras</papertitle>
        </a>
        <br>
        <strong>Wei Yang</strong>,
        Yingliang Zhang,
        <a href="https://cs.gmu.edu/~jinweiye/">Jinwei Ye</a>,
        <a href="https://yeauxji.github.io/">Yu Ji</a>,
        <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
        Mingyuan Zhou,
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>TPAMI</em>, 2019 &nbsp;
        <br> 
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8918483">Paper</a>
        <p></p>
        <p>We present a structure-from-motion (SfM) framework based on a special type of multi-perspective camera called the cross-slit or XSlit camera.</p>
      </td>
    </tr>

    <!-- Publication Section: dhuman -->
    <tr onmouseout="dhuman_stop()" onmouseover="dhuman_start()">
          <td align="center" valign="middle"  width="25%">
            <div class="one">
              <div class="two_vc" id="dhuman_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
                <source src="./pub/dhuman.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>              
                <img id="dhuman_static_image" src="./pub/dhuman.png" width="160">
            </div>
            <script type="text/javascript">
              function dhuman_start() {
                document.getElementById('dhuman_static_image').style.opacity = "0";
                document.getElementById('dhuman_image').style.opacity = "1";
              }
    
              function dhuman_stop() {
                document.getElementById('dhuman_image').style.opacity = "0";
                document.getElementById('dhuman_static_image').style.opacity = "1";
              }
              dhuman_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8918483">
              <papertitle>Robust 3D Human Motion Reconstruction via Dynamic Template Construction</papertitle>
            </a>
            <br>
            <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
            <a href="https://yeauxji.github.io/">Yu Ji</a>,
            <strong>Wei Yang</strong>,
            <a href="https://cs.gmu.edu/~jinweiye/">Jinwei Ye</a>,
            <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
            <br>
            <em>3DV</em>, 2017 &nbsp;<font color="tomato"><strong>(Spotlight Oral Presentation)</strong></font>
            <br> 
            <a href="https://arxiv.org/abs/1801.10434">arXiv</a>
            /
            <a href="https://www.youtube.com/watch?v=nYXG7kxvM3c&ab_channel=ZhongLi">Video</a>
            /
            <a href="https://vic.shanghaitech.edu.cn/vrvc/en/Code.html">Data</a>
            <p></p>
            <p>We generate a global full-body template by registering all poses in the acquired motion sequence, and then construct a deformable graph by utilizing the rigid components in the global template.</p>
          </td>
        </tr>

    <!-- Publication Section: LF Scanner -->
    <tr onmouseout="lfscanner_stop()" onmouseover="lfscanner_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="lfscanner_image" style="opacity: 0;"><img src="./pub/lfscanner.png" width="150"></div>
          <img id="lfscanner_static_image" src="./pub/lfscanner.png" width="150">
        </div>
        <script type="text/javascript">
          function lfscanner_start() {
            document.getElementById('lfscanner_image').style.opacity = "1";
          }

          function lfscanner_stop() {
            document.getElementById('lfscanner_image').style.opacity = "0";
          }
          lfscanner_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://ieeexplore.ieee.org/document/7951484">
          <papertitle>The Light Field 3D Scanner</papertitle>
        </a>
        <br>
        Yingliang Zhang,
        <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
        <strong>Wei Yang</strong>,
        <a href="https://peihongyu.com/">Peihong Yu</a>,
        Haiting Lin,
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>ICCP</em>, 2017 &nbsp;
        <br> 
        <a href="https://ieeexplore.ieee.org/document/7951484">Paper</a>
        <p></p>
        <p>We use the light field (LF) camera such as Lytro and Raytrix as a virtual 3D scanner.</p>
      </td>
    </tr>

    <!-- Publication Section: rayspace -->
    <tr onmouseout="rayspace_stop()" onmouseover="rayspace_start()">
          <td align="center" valign="middle"  width="25%">
            <div class="one">
              <div class="two" id="rayspace_image" style="opacity: 0;"><img src="./pub/rayspace.png" width="150"></div>
              <img id="rayspace_static_image" src="./pub/rayspace.png" width="150">
            </div>
            <script type="text/javascript">
              function rayspace_start() {
                document.getElementById('rayspace_image').style.opacity = "1";
              }
    
              function rayspace_stop() {
                document.getElementById('rayspace_image').style.opacity = "0";
              }
              rayspace_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Ray_Space_Features_ICCV_2017_paper.pdf">
              <papertitle>Ray Space Features for Plenoptic Structure-from-Motion</papertitle>
            </a>
            <br>
            Yingliang Zhang,
            <a href="https://peihongyu.com/">Peihong Yu</a>,
            <strong>Wei Yang</strong>,
            Yuanxi Ma,
            <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
            <br>
            <em>ICCV</em>, 2017 &nbsp;
            <br> 
            <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Ray_Space_Features_ICCV_2017_paper.pdf">Paper</a>
            <p></p>
            <p>We present a comprehensive theory on ray geometry transforms under light field pose variations, and derive the transforms of three typical ray manifolds.</p>
          </td>
        </tr>

    <!-- Publication Section: xara -->
    <tr onmouseout="xara_stop()" onmouseover="xara_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="xara_image" style="opacity: 0;"><img src="./pub/xara.png" width="150"></div>
          <img id="xara_static_image" src="./pub/xara.png" width="150">
        </div>
        <script type="text/javascript">
          function xara_start() {
            document.getElementById('xara_image').style.opacity = "1";
          }

          function xara_stop() {
            document.getElementById('xara_image').style.opacity = "0";
          }
          xara_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Resolving_Scale_Ambiguity_ICCV_2015_paper.pdf">
          <papertitle>Resolving Scale Ambiguity via XSlit Aspect Ratio Analysis</papertitle>
        </a>
        <br>
        <strong>Wei Yang</strong>,
        Haiting Lin,
        <a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>ICCV</em>, 2015 &nbsp;
        <br> 
        <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Resolving_Scale_Ambiguity_ICCV_2015_paper.pdf">Paper</a>
        <p></p>
        <p> We present the depth dependent aspecratio (DDAR) property that can be used to 3D recovery.</p>
      </td>
    </tr>
    
    <!-- Publication Section: AO -->
    <tr onmouseout="ao_stop()" onmouseover="ao_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="ao_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/ao.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>              
            <img id="ao_static_image" src="./pub/ao.png" width="140">
        </div>
        <script type="text/javascript">
          function ao_start() {
            document.getElementById('ao_static_image').style.opacity = "0";
            document.getElementById('ao_image').style.opacity = "1";
          }

          function ao_stop() {
            document.getElementById('ao_image').style.opacity = "0";
            document.getElementById('ao_static_image').style.opacity = "1";
          }
          ao_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Yang_Ambient_Occlusion_via_2015_CVPR_paper.pdf">
          <papertitle>Ambient Occlusion via Compressive Visibility Estimation</papertitle>
        </a>
        <br>
        <strong>Wei Yang</strong>,
        <a href="https://yeauxji.github.io/">Yu Ji</a>,
        Haiting Lin,
        Yang Yang,
        <a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>CVPR</em>, 2015 &nbsp;
        <br> 
        <a href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Yang_Ambient_Occlusion_via_2015_CVPR_paper.pdf">Paper</a>
        <p></p>
        <p>We present a novel computational imaging solution for recovering AO by adopting a compressive sensing framework.</p>
      </td>
    </tr>

    <!-- Publication Section: xdof -->
    <tr onmouseout="xdof_stop()" onmouseover="xdof_start()">
          <td align="center" valign="middle"  width="25%">
            <div class="one">
              <div class="two" id="xdof_image" style="opacity: 0;"><img src="./pub/xdof.png" width="160"></div>
              <img id="xdof_static_image" src="./pub/xdof.png" width="160">
            </div>
            <script type="text/javascript">
              function xdof_start() {
                document.getElementById('xdof_image').style.opacity = "1";
              }
    
              function xdof_stop() {
                document.getElementById('xdof_image').style.opacity = "0";
              }
              xdof_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ECCV-2014/papers/8691/86910753.pdf">
              <papertitle>Depth-of-field and Coded Aperture Imaging on Xslit Lens</papertitle>
            </a>
            <br>
            <a href="https://cs.gmu.edu/~jinweiye/">Jinwei Ye</a>,
            <a href="https://yeauxji.github.io/">Yu Ji</a>,
            <strong>Wei Yang</strong>,
            <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
            <br>
            <em>ECCV</em>, 2014 &nbsp;
            <br> 
            <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ECCV-2014/papers/8691/86910753.pdf">Paper</a>
            /
            <a href="eccv2014_yu_coded_aperture">Video</a>
            <p></p>
            <p>We explore coded aperture solutions on a special non-centric lens called the crossedslit (XSlit) lens.</p>
          </td>
      </tr>
    <!-- Publication Section: ccp -->
    <tr onmouseout="ccp_stop()" onmouseover="ccp_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="ccp_image" style="opacity: 0;"><img src="./pub/ccp.png" width="160"></div>
          <img id="ccp_static_image" src="./pub/ccp.png" width="160">
        </div>
        <script type="text/javascript">
          function ccp_start() {
            document.getElementById('ccp_image').style.opacity = "1";
          }

          function ccp_stop() {
            document.getElementById('ccp_image').style.opacity = "0";
          }
          ccp_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ECCV-2014/papers/8689/86890220.pdf">
          <papertitle>Coplanar Common Points in Non-centric Cameras</papertitle>
        </a>
        <br>
        <strong>Wei Yang</strong>,
        <a href="https://yeauxji.github.io/">Yu Ji</a>,
        <a href="https://cs.gmu.edu/~jinweiye/">Jinwei Ye</a>,
        S. Susan Young,
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>ECCV</em>, 2014 &nbsp;
        <br> 
        <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ECCV-2014/papers/8689/86890220.pdf">Paper</a>
        <p></p>
        <p>We address the problem of determining CCP existence in general non-centric cameras.</p>
      </td>
  </tr>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          I borrowed this website template from <a href="https://jonbarron.info/">Jon Barron</a>, thanks! <br>
	    </font>
        </p>
        </td>
      </tr>
      </tbody></table>

    </td>
    </tr>
  </tbody></table>
  

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0035)https://weiyang-hust.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="“width=800”">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

a {
color: #1772d0;
text-decoration: none;
}

a:focus,
a:hover {
color: #f09228;
text-decoration: none;
}

body,
td,
th,
tr,
p,
a {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px
}

strong {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
}

heading {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 22px;
}

papertitle {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
font-weight: 700
}

name {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 32px;
font-weight: bold;
}

affliation {
  font-family: 'Lato', Verdana, Helvetica, sans-serif;
  font-size: 14px;  
  font-style: oblique;
}

.one {
width: 160px;
height: auto;
position: relative;
}

.one_vc {
  margin: 0;
  position: relative;
  top: 50%;
  -ms-transform: translateY(-50%);
  transform: translateY(-50%);
}

.two {
width: 160px;
height: 160px;
position: absolute;
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

.two_vc {
  width: 160px;
  height: 160px;
  z-index: 2;
  transition: opacity .2s ease-in-out;
  -moz-transition: opacity .2s ease-in-out;
  -webkit-transition: opacity .2s ease-in-out;
  position: absolute;
  top: 50%;
  -ms-transform: translateY(-50%);
  transform: translateY(-50%);
}

.fade {
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

span.highlight {
background-color: #ffffd0;
}
  </style>
  <link rel="icon" type="image/png" href="./img/logo.png">
  <title>Wei Yang</title>
  
  <link href="./wei_yang_files/css" rel="stylesheet" type="text/css">

  </head>
  <body data-new-gr-c-s-check-loaded="14.1101.0" data-gr-ext-installed="">
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="67%" valign="middle">
        <p align="left">
          <name>Wei Yang (杨卫)</name>
        </p>
        <p align="left">
          <affliation> Associate Professor<br>
            School of Computer Science & Technology, HUST<br>
            Room 306, Southern Building #6            
            </affliation>
          </p>
          <p>
            I am an Associate Professor at the <a href="http://english.cs.hust.edu.cn/">School of Computer Science, Huazhong University of Science and Technology</a>, where I co-lead the <a href="http://media.hust.edu.cn/">HUST Media Lab</a>. My research interests primarily lie in the areas of imaging, graphics, computer vision, and artificial intelligence.
        </p>
        <p>
            Before joining HUST, I worked in the <a href="https://en.wikipedia.org/wiki/Google_ATAP">Advanced Technology and Projects (ATAP)</a> division at <a href="https://about.google/">Google</a> in Mountain View, USA. As a proud member of the Te'veren team, I collaborated with Rick Marks on advanced sensing and on-device intelligence using computer vision. Prior to Google, I served as a Principal Scientist at <a href="https://www.us1.dgene.com/">DGene, US</a>, where I conducted research on real-time volumetric human capture systems.
        </p>
        <p>
            I graduated from the <a href="https://udel.edu">University of Delaware</a> in 2017, where I majored in Computer Science. At UDel, I worked with Professor <a href="http://www.yu-jingyi.com/">Jingyi Yu</a> on research problems in computational photography and scene understanding. During my PhD, I interned at Adobe, hosted by the ACR team, in 2015.
        </p>
        <p>
          <font color="red"><strong>I am actively seeking creative and highly motivated MS and PhD students who are passionate about research.</strong></font>
        </p>
        <p align="center">
          <a href="mailto:weiyangcs@hust.edu.cn">Email</a> &nbsp;/&nbsp;
	      <a href="https://scholar.google.com/citations?hl=en&user=rhsNpmkAAAAJ">Google Scholar</a> &nbsp;
        </p>
        </td>
        <td width="33%">
	  <a href="https://weiyang-hust.github.io/img/avatar_narrow.png">
        <img src="./img/avatar_narrow.png" width="250px"></a>
        </td>
      </tr>
      </tbody></table>
    
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td width="100%" valign="middle">
            <heading>Services</heading>
            <ul><li><strong>Area Chair</strong>: ICML 25, ICCV 25, CVPR 25, NeurIPS 24, CVPR 24, NeurIPS 23, CVPR 23</li></ul>
            <ul><li><strong>Program Committee</strong>: AAAI 20, AAAI 21, WACV 21, BMVC 18</li></ul>
            <ul><li><strong>Reviewer</strong>: CVPR, ICCV, NeurIPS, ICML, ICLR, ECCV, TPAMI, TIP, TVCG...</li></ul>
          </td>
        </tr>
        </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
          <heading>Research and Publications</heading>
          <p>
            <sup>&#134</sup> denotes corresponding author
          </p>
        </td>
      </tr>
      </tbody></table>
  
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    
    <tbody>

    <!-- Publication Section: refgs -->
    <tr onmouseout="refgs_stop()" onmouseover="refgs_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="refgs_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/refgs.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="refgs_static_image" src="./pub/refgs.png" width="160">
        </div>
        <script type="text/javascript">
          function refgs_start() {
            document.getElementById('refgs_static_image').style.opacity = "0";
            document.getElementById('refgs_image').style.opacity = "1";
          }

          function refgs_stop() {
            document.getElementById('refgs_image').style.opacity = "0";
            document.getElementById('refgs_static_image').style.opacity = "1";
          }
          refgs_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2412.00905">
          <papertitle>Ref-GS: Directional Factorization for 2D Gaussian Splatting</papertitle>
        </a>
        <br>
        <a href="https://youjiazhang.github.io/">Youjia Zhang</a>, 
        <a href="https://apchenstu.github.io/">Anpei Chen</a><sup>&#134</sup>, 
        Yumin Wan, 
        <a href="https://skyesong38.github.io/">Zikai Song</a>,
        Junqing Yu, 
        <a href="https://github.com/RoyalVane">Yawei Luo</a>
        <strong>Wei Yang</strong><sup>&#134</sup>,
        <br>
        <em>CVPR</em>, 2025 &nbsp;
        <br> 
        <a href="https://arxiv.org/abs/2412.00905">Paper</a>
        /
        <a href="https://ref-gs.github.io/">Project</a>
        <p></p>
        <p>Ref-GS builds upon the deferred rendering of Gaussian splatting and applies directional encoding to the deferred-rendered surface, effectively reducing the ambiguity between orientation and viewing angle. We introduce a spherical Mip-grid to capture varying levels of surface roughness, enabling roughness-aware Gaussian shading.</p>
      </td>
    </tr>

    <!-- Publication Section: SF2T -->
    <tr onmouseout="sf2t_stop()" onmouseover="sf2t_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="sf2t_image" style="opacity: 0;"><img src="./pub/sf2t.png" width="160"></div>
          <img id="sf2t_static_image" src="./pub/sf2t_0.png" width="160">
        </div>
        <script type="text/javascript">
          function sf2t_start() {
            document.getElementById('sf2t_image').style.opacity = "1";
          }

          function sf2t_stop() {
            document.getElementById('sf2t_image').style.opacity = "0";
          }
          sf2t_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://cvpr.thecvf.com/virtual/2025/poster/33524">
          <papertitle>SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding</papertitle>
        </a>
        <br>
        Yangliu Hu, 
        <a href="https://skyesong38.github.io/">Zikai Song</a><sup>&#134</sup>,
        Na Feng, 
        <a href="https://github.com/RoyalVane">Yawei Luo</a>
        Junqing Yu
        Yi-Ping Phoebe Chen, 
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>CVPR</em>, 2025&nbsp;
        <br> 
        <a href="https://cvpr.thecvf.com/virtual/2025/poster/33524">Paper</a>
        /
        <a href="xxx">Project</a>
        <p></p>
        <p>We propose the Self-Supervised Fragment Fine-Tuning (SF^2T), a novel effortless fine-tuning method, employs the rich inherent characteristics of videos for training, while unlocking more fine-grained understanding ability of Video-LLMs, along with a novel benchmark dataset, namely FineVidBench, for rigorously assessing Video-LLMs' performance at both the scene and fragment levels, offering a comprehensive evaluation of their capabilities.</p>
      </td>
    </tr>

    <!-- Publication Section: CLAY -->
    <tr onmouseout="clay_stop()" onmouseover="clay_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="clay_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/clay.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="clay_static_image" src="./pub/clay.jpg" width="160">
        </div>
        <script type="text/javascript">
          function clay_start() {
            document.getElementById('clay_static_image').style.opacity = "0";
            document.getElementById('clay_image').style.opacity = "1";
          }

          function clay_stop() {
            document.getElementById('clay_image').style.opacity = "0";
            document.getElementById('clay_static_image').style.opacity = "1";
          }
          clay_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3658146">
          <papertitle>CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets</papertitle>
        </a>
        <br>
        <a href="https://zhanglongwen.com/">Longwen Zhang</a>, 
        Ziyu Wang,
        Qixuan Zhang,
        Qiwei Qiu,
        Anqi Pang, 
        Haoran Jiang
        <strong>Wei Yang</strong>,
        <a href="http://xu-lan.com/">Lan Xu</a>, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>SIGGRAPH</em>, 2024 &nbsp; <font color="tomato"><strong>(Honorable Mentions)</strong></font>
        <br> 
        <a href="https://github.com/CLAY-3D/OpenCLAY">Project</a>
        /
        <a href="https://dl.acm.org/doi/pdf/10.1145/3658146">Paper</a>
        /
        <a href="https://hyper3d.ai/">Website</a>
        <p></p>
        <p>We introduce CLAY, a 3D geometry and material generator designed to effortlessly transform human imagination into intricate 3D digital structures. CLAY supports classic text or image inputs as well as 3D-aware controls from diverse primitives (multi-view images, voxels, bounding boxes, point clouds, implicit representations, etc).</p>
      </td>
    </tr>

    <!-- Publication Section: gpd -->
    <tr onmouseout="gpd_stop()" onmouseover="gpd_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="gpd_image" style="opacity: 0;"><img src="./pub/gpd.png" width="160"></div>
          <img id="gpd_static_image" src="./pub/gpd.png" width="160">
        </div>
        <script type="text/javascript">
          function gpd_start() {
            document.getElementById('gpd_image').style.opacity = "1";
          }

          function gpd_stop() {
            document.getElementById('gpd_image').style.opacity = "0";
          }
          gpd_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2412.09026">
          <papertitle>Video Anomaly Detection with Motion and Appearance Guided Patch Diffusion Model</papertitle>
        </a>
        <br>
        Hang Zhou, 
        Jiale Cai, 
        Yuteng Ye, 
        Yonghui Feng, 
        Chenxing Gao, 
        Junqing Yu, 
        <a href="https://skyesong38.github.io/">Zikai Song</a><sup>&#134</sup>,
        <strong>Wei Yang</strong>
        <br>
        <em>ACM MM</em>, 2024&nbsp;
        <br> 
        <a href="https://arxiv.org/abs/2412.09026">Paper</a>
        <p></p>
        <p>We introduce innovative motion and appearance conditions that are seamlessly integrated into a patch diffusion model to guide the model in generating coherent and contextually appropriate predictions for both semantic content and motion relations.</p>
      </td>
    </tr>

    <!-- Publication Section: ALTrack -->
    <tr onmouseout="altrack_stop()" onmouseover="altrack_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="altrack_image" style="opacity: 0;"><img src="./pub/altrack.png" width="160"></div>
          <img id="altrack_static_image" src="./pub/altrack.png" width="160">
        </div>
        <script type="text/javascript">
          function altrack_start() {
            document.getElementById('altrack_image').style.opacity = "1";
          }

          function altrack_stop() {
            document.getElementById('altrack_image').style.opacity = "0";
          }
          altrack_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2407.20730">
          <papertitle>Autogenic Language Embedding for Coherent Point Tracking</papertitle>
        </a>
        <br>
        <a href="https://skyesong38.github.io/">Zikai Song</a>,
        Ying Tang, 
        Run Luo, 
        Lintao Ma, 
        Junqing Yu<sup>&#134</sup>, 
        Yi-Ping Phoebe Chen, 
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>ACM MM</em>, 2024&nbsp;
        <br> 
        <a href="https://arxiv.org/abs/2407.20730">Paper</a>
        /
        <a href="https://github.com/SkyeSong38/ALTrack">Project</a>
        <p></p>
        <p>ALTrack is a coherent point tracking framework which designes an autogenic language embedding for visual feature enhancement, strengthens point correspondence in long-term sequences. Unlike existing visual-language schemes, our approach learns text embeddings from visual features through a dedicated mapping network, enabling seamless adaptation to various tracking tasks without explicit text annotations.</p>
      </td>
    </tr>
    
    <!-- Publication Section: CoupledMamaba -->
    <tr onmouseout="coupledmamba_stop()" onmouseover="coupledmamba_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="coupledmamba_image" style="opacity: 0;"><img src="./pub/coupledmamba_0.png" width="160"></div>
          <img id="coupledmamba_static_image" src="./pub/coupledmamba_0.png" width="160">
        </div>
        <script type="text/javascript">
          function coupledmamba_start() {
            document.getElementById('coupledmamba_image').style.opacity = "1";
          }

          function coupledmamba_stop() {
            document.getElementById('coupledmamba_image').style.opacity = "0";
          }
          coupledmamba_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2405.18014">
          <papertitle>Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model</papertitle>
        </a>
        <br>
        Wenbing Li,
        Hang Zhou,
        Junqing Yu
        <a href="https://skyesong38.github.io/">Zikai Song</a><sup>&#134</sup>,
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>NeurIPS</em>, 2024 &nbsp;
        <br> 
        <a href="https://arxiv.org/abs/2405.18014">Paper</a>
        /
        <a href="https://github.com/hustcselwb/coupled-mamba">Code</a>
        <p></p>
        <p>We propose the Coupled SSM model, which links the state chains of multiple modalities while keeping intra-modality processes independent. Our model includes an inter-modal state transition mechanism, where the current state depends on both its own chain and neighboring chains' states from the previous time step.</p>
      </td>
    </tr>

    <!-- Publication Section: TIGER -->
    <tr onmouseout="tiger_stop()" onmouseover="tiger_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="tiger_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/tiger.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="tiger_static_image" src="./pub/tiger.png" width="160">
        </div>
        <script type="text/javascript">
          function tiger_start() {
            document.getElementById('tiger_static_image').style.opacity = "0";
            document.getElementById('tiger_image').style.opacity = "1";
          }

          function tiger_stop() {
            document.getElementById('tiger_image').style.opacity = "0";
            document.getElementById('tiger_static_image').style.opacity = "1";
          }
          tiger_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2405.14455">
          <papertitle>TIGER: Text-Instructed 3D Gaussian Retrieval and Coherent Editing</papertitle>
        </a>
        <br>
        Teng Xu,
        Jiamin Chen, 
        Peng Chen,
        <a href="https://youjiazhang.github.io/">Youjia Zhang</a>, 
        Junqing Yu,
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>ArXiv</em>, 2024 &nbsp;
        <br> 
        <a href="https://xutanxing.github.io/TIGER/">Project</a>
        /
        <a href="https://arxiv.org/abs/2405.14455">Paper</a>
        <p></p>
        <p>We propose a Coherent Score Distillation (CSD) that aggregates a 2D image editing diffusion model and a multi-view diffusion model for score distillation, producing multi-view consistent editing with much finer details.</p>
      </td>
    </tr>

    <!-- Publication Section: AMD -->
    <tr onmouseout="amd_stop()" onmouseover="amd_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="amd_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/AMD.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="amd_static_image" src="./pub/amd.png" width="160">
        </div>
        <script type="text/javascript">
          function amd_start() {
            document.getElementById('amd_static_image').style.opacity = "0";
            document.getElementById('amd_image').style.opacity = "1";
          }

          function amd_stop() {
            document.getElementById('amd_image').style.opacity = "0";
            document.getElementById('amd_static_image').style.opacity = "1";
          }
          amd_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2312.12763">
          <papertitle>AMD: Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion</papertitle>
        </a>
        <br>
        Beibei Jing,
        <a href="https://youjiazhang.github.io/">Youjia Zhang</a>, 
        <a href="https://skyesong38.github.io/">Zikai Song</a>,
        Junqing Yu,
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>AAAI</em>, 2024 &nbsp;
        <br> 
        <a href="https://arxiv.org/abs/2312.12763">Paper</a>
        /
        <a href="xxxx">Code Comming Soon</a>
        <p></p>
        <p>We propose the Adaptable Motion Diffusion (AMD) model, which leverages a Large Language Model (LLM) to parse the input text into a sequence of concise and interpretable anatomical scripts that correspond to the target motion.</p>
      </td>
    </tr>

    <!-- Publication Section: PT2I -->
    <tr onmouseout="pt2i_stop()" onmouseover="pt2i_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="pt2i_image" style="opacity: 0;"><img src="./pub/pt2i.png" width="160"></div>
          <img id="pt2i_static_image" src="./pub/pt2i.png" width="160">
        </div>
        <script type="text/javascript">
          function pt2i_start() {
            document.getElementById('pt2i_image').style.opacity = "1";
          }

          function pt2i_stop() {
            document.getElementById('pt2i_image').style.opacity = "0";
          }
          pt2i_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2309.09466">
          <papertitle>Progressive Text-to-Image Diffusion with Soft Latent Direction</papertitle>
        </a>
        <br>
        Yuteng Ye,
        Jiale Cai,
        Hang Zhou,
        Guanwen Li,
        <a href="https://youjiazhang.github.io/">Youjia Zhang</a>, 
        <a href="https://skyesong38.github.io/">Zikai Song</a>,
        Chenxing Gao,
        Junqing Yu,
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>AAAI</em>, 2024 &nbsp;
        <br> 
        <a href="https://arxiv.org/abs/2309.09466">Paper</a>
        /
        <a href="https://github.com/babahui/Progressive-Text-to-Image">Code</a>
        <p></p>
        <p> We propose to harness the capabilities of a Large Language Model (LLM) to decompose text descriptions into coherent directives adhering to stringent formats and progressively generate the target image.</p>
      </td>
    </tr>

    <!-- Publication Section: FDAP -->
    <tr onmouseout="fdap_stop()" onmouseover="fdap_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="fdap_image" style="opacity: 0;"><img src="./pub/fdap.png" width="150"></div>
          <img id="fdap_static_image" src="./pub/fdap.png" width="150">
        </div>
        <script type="text/javascript">
          function fdap_start() {
            document.getElementById('fdap_image').style.opacity = "1";
          }

          function fdap_stop() {
            document.getElementById('fdap_image').style.opacity = "0";
          }
          fdap_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/xxx.xxx">
          <papertitle>Attacking Transformers with Feature Diversity Adversarial Perturbation</papertitle>
        </a>
        <br>
        Chenxing Gao,
        Hang Zhou,
        Junqing Yu,
        YuTeng Ye,
        Jiale Cai,
        Junle Wang,
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>AAAI</em>, 2024 &nbsp;
        <br> 
        <a href="https://arxiv.org/abs/2211.14742">Paper Cooming Soon</a>
        <p></p>
        <p>We present a label-free white-box attack approach for ViT-based models that exhibits strong transferability to various black-box models by accelerating the feature collapse.</p>
      </td>
    </tr>

    <!-- Publication Section: DFP -->
    <tr onmouseout="dfp_stop()" onmouseover="dfp_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="dfp_image" style="opacity: 0;"><img src="./pub/dfp.png" width="160"></div>
          <img id="dfp_static_image" src="./pub/dfp.png" width="160">
        </div>
        <script type="text/javascript">
          function dfp_start() {
            document.getElementById('dfp_image').style.opacity = "1";
          }

          function dfp_stop() {
            document.getElementById('dfp_image').style.opacity = "0";
          }
          dfp_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2211.14742">
          <papertitle>Dynamic Feature Pruning and Consolidation for Occluded Person Re-Identification</papertitle>
        </a>
        <br>
        Yuteng Ye,
        Hang Zhou,
        Junqing Yu,
        <a href="https://qianghu-huber.github.io/qianghuhomepage/">Qiang Hu</a>,
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>AAAI</em>, 2024 &nbsp;
        <br> 
        <a href="https://arxiv.org/abs/2211.14742">Paper</a>
        /
        <a href="https://github.com/babahui/FPC">Code</a>
        <p></p>
        <p> We propose a Feature Pruning and Consolidation (FPC) framework to circumvent explicit human structure parse, which consists of a sparse encoder, a global and local feature ranking module, and a feature consolidation decoder.</p>
      </td>
    </tr>

    <!-- Publication Section: difftrack -->
    <tr onmouseout="difftrack_stop()" onmouseover="difftrack_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="difftrack_image" style="opacity: 0;"><img src="./pub/difftrack.png" width="160"></div>
          <img id="difftrack_static_image" src="./pub/difftrack.png" width="160">
        </div>
        <script type="text/javascript">
          function difftrack_start() {
            document.getElementById('difftrack_image').style.opacity = "1";
          }

          function difftrack_stop() {
            document.getElementById('difftrack_image').style.opacity = "0";
          }
          difftrack_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2308.09905">
          <papertitle>DiffusionTrack: Diffusion Model For Multi-Object Tracking</papertitle>
        </a>
        <br>
        Run Luo, 
        <a href="https://skyesong38.github.io/">Zikai Song</a>,
        Lintao Ma, 
        Jinlin Wei, 
        <strong>Wei Yang</strong>,
        <a href="https://minyang.me/">Min Yang</a>
        <br>
        <em>AAAI</em>, 2024 &nbsp;
        <br> 
        <a href="https://arxiv.org/abs/2308.09905">Paper</a>
        /
        <a href="https://github.com/RainBowLuoCS/DiffusionTrack">Code</a>
        <p></p>
        <p> We formulates object detection and association jointly as a consistent denoising diffusion process from paired noise boxes to paired ground-truth boxes.</p>
      </td>
    </tr>

    <!-- Publication Section: NeMF -->
    <tr onmouseout="nemf_stop()" onmouseover="nemf_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="nemf_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/NeMF.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="nemf_static_image" src="./pub/NeMF.png" width="160">
        </div>
        <script type="text/javascript">
          function nemf_start() {
            document.getElementById('nemf_static_image').style.opacity = "0";
            document.getElementById('nemf_image').style.opacity = "1";
          }

          function nemf_stop() {
            document.getElementById('nemf_image').style.opacity = "0";
            document.getElementById('nemf_static_image').style.opacity = "1";
          }
          nemf_stop()
        </script>
      </td>

      <td valign="middle" width="75%">
        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_NeMF_Inverse_Volume_Rendering_with_Neural_Microflake_Field_ICCV_2023_paper.pdf">
          <papertitle>NeMF: Inverse Volume Rendering with Neural Microflake Field</papertitle>
        </a>
        <br>
        <a href="https://youjiazhang.github.io/">Youjia Zhang</a>, 
        Teng Xu, 
        Junqing Yu, 
        Yuteng Ye, 
        Yanqing Jing, 
        Junle Wang, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>ICCV</em>, 2023&nbsp;
        <br>
        <a href="https://arxiv.org/abs/2304.00782">arXiv</a>
        /
        <a href="https://github.com/YoujiaZhang/NeMF">Project Page</a>
        <p></p>
        <p>We propose to conduct inverse volume rendering by representing a scene using microflake volume, which assumes the space is filled with infinite small flakes and light reflects or scattersat each spatial location according to microflake distributions.</p>
      </td>
    </tr>

    <!-- Publication Section: c2f2neus -->
    <tr onmouseout="c2f2neus_stop()" onmouseover="c2f2neus_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="c2f2neus_image" style="opacity: 0;"><img src="./pub/c2f2neus.png" width="160"></div>
          <img id="c2f2neus_static_image" src="./pub/c2f2neus.png" width="160">
        </div>
        <script type="text/javascript">
          function c2f2neus_start() {
            document.getElementById('c2f2neus_image').style.opacity = "1";
          }

          function c2f2neus_stop() {
            document.getElementById('c2f2neus_image').style.opacity = "0";
          }
          c2f2neus_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2306.10003">
          <papertitle>C2F2NeUS: Cascade Cost Frustum Fusion for High Fidelity and Generalizable Neural Surface Reconstruction</papertitle>
        </a>
        <br>
        Luoyuan Xu, 
        Tao Guan, 
        Yuesong Wang, 
        Wenkai Liu, 
        Zhaojie Zeng, 
        Junle Wang, 
        <strong>Wei Yang</strong>
        <br>
        <em>ICCV</em>, 2023 &nbsp;
        <br> 
        <a href="https://arxiv.org/abs/2306.10003">Paper</a>
        <p></p>
        <p> We propose to construct per-view cost frustum and fuse cross-view frustums for finer geometry estimation.</p>
      </td>
    </tr>

    <!-- Publication Section: DreamFace -->
    <tr onmouseout="dreamface_stop()" onmouseover="dreamface_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="dreamface_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/DreamFace.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="dreamface_static_image" src="./pub/DreamFace.png" width="160">
        </div>
        <script type="text/javascript">
          function dreamface_start() {
            document.getElementById('dreamface_static_image').style.opacity = "0";
            document.getElementById('dreamface_image').style.opacity = "1";
          }

          function dreamface_stop() {
            document.getElementById('dreamface_image').style.opacity = "0";
            document.getElementById('dreamface_static_image').style.opacity = "1";
          }
          dreamface_stop()
        </script>
      </td>

      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2304.03117">
          <papertitle>DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance</papertitle>
        </a>
        <br>
        <a href="https://zhanglongwen.com/">Longwen Zhang</a>, 
        Qiwei Qiu,
        Hongyang Lin,
        Qixuan Zhang, 
        Cheng Shi, 
        <strong>Wei Yang</strong>,
        <a href="https://sist.shanghaitech.edu.cn/sist_en/2020/0707/c7582a59078/page.htm">Ye Shi</a>, 
        <a href="https://sist.shanghaitech.edu.cn/sist_en/2021/0630/c7582a66988/page.htm">Sibei Yang</a>, 
        <a href="http://xu-lan.com/">Lan Xu</a>, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>SIGGRAPH</em>, 2023&nbsp;
        <br>
        <a href="https://arxiv.org/pdf/2304.03117.pdf">arXiv</a>
        /
        <a href="https://sites.google.com/view/dreamface">Project Page</a>
        /
        <a href="https://www.youtube.com/watch?v=w4UzPaGssMM&ab_channel=ShanghaiTechDigitalHuman">Video</a>
        /
        <a href="https://hyperhuman.deemos.com/chatavatar">Web Demo</a>
        /
        <a href="https://huggingface.co/spaces/DEEMOSTECH/ChatAvatar">Huggingface Space</a>
        <p></p>
        <p>DreamFace is a progressive scheme to generate personalized 3D faces under text guidance.</p>
      </td>
    </tr>

    <!-- Publication Section: HACK -->
    <tr onmouseout="hack_stop()" onmouseover="hack_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="hack_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/HACK.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="hack_static_image" src="./pub/HACK.png" width="160">
        </div>
        <script type="text/javascript">
          function hack_start() {
            document.getElementById('hack_static_image').style.opacity = "0";
            document.getElementById('hack_image').style.opacity = "1";
          }

          function hack_stop() {
            document.getElementById('hack_image').style.opacity = "0";
            document.getElementById('hack_static_image').style.opacity = "1";
          }
          hack_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2305.04469">
          <papertitle>HACK: Learning a Parametric Head and Neck Model for High-fidelity Animation</papertitle>
        </a>
        <br>
        <a href="https://zhanglongwen.com/">Longwen Zhang</a>, 
        Zijun Zhao, 
        Xinzhou Cong, 
        Qixuan Zhang, 
        Shuqi Gu,
        Yuchong Gao,
        <a href="https://sist.shanghaitech.edu.cn/sist_en/2020/0814/c7582a54817/page.htm">Rui Zheng</a>,
        <strong>Wei Yang</strong>,
        <a href="http://xu-lan.com/">Lan Xu</a>, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>SIGGRAPH</em>, 2023&nbsp;
        <br>
        <a href="https://arxiv.org/pdf/2305.04469.pdf">arXiv</a>
        /
        <a href="https://sites.google.com/view/hack-model">Project Page</a>
        /
        <a href="https://www.youtube.com/watch?v=0TCW0lPgK0E&ab_channel=ShanghaiTechDigitalHuman">Video</a>
        <p></p>
        <p>We introduce HACK (Head-And-neCK), a novel parametric model for constructing the head and cervical region of
          digital humans.</p>
      </td>
    </tr>

      <!-- Publication Section: ADP -->
    <tr onmouseout="adp_stop()" onmouseover="adp_start()">
      <td valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="adp_image" style="opacity: 0;"><img src="./pub/adp.gif" width="160"></div>
          <img src="./pub/adp.png" width="160">
        </div>
        <script type="text/javascript">
          function adp_start() {
            document.getElementById('adp_image').style.opacity = "1";
          }

          function adp_stop() {
            document.getElementById('adp_image').style.opacity = "0";
          }
          adp_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://openreview.net/forum?id=sc3sKQqszx">
          <papertitle>Adaptive Patch Deformation for Textureless-Resilient Multi-View Stereo</papertitle>
        </a>
        <br>
        Yuesong Wang, 
        Zhaojie Zeng,
        Tao Guan,
        <strong>Wei Yang</strong>,
        Zhuo Chen,
        Wenkai Liu,
        Luoyuan Xu,
        <a href="https://github.com/RoyalVane">Yawei Luo</a>
        <br>
        <em>CVPR</em>, 2023 &nbsp;
        <br>
        <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Adaptive_Patch_Deformation_for_Textureless-Resilient_Multi-View_Stereo_CVPR_2023_paper.pdf">Paper</a>
        /
        <a href="https://github.com/whoiszzj/APD-MVS">Code</a>
        <p></p>
        <p>We transplant the spirit of deformable convolution into the PatchMatch-based method for both memory-friendly and textureless-resilient MVS.</p>
      </td>
    </tr>

    <!-- Publication Section: CTT -->
    <tr onmouseout="cttrack_stop()" onmouseover="cttrack_start()">
      <td valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="cttrack_image" style="opacity: 0;"><img src="./pub/cttrack.png" width="160"></div>
          <img src="./pub/cttrack.png" width="160">
        </div>
        <script type="text/javascript">
          function cttrack_start() {
            document.getElementById('cttrack_image').style.opacity = "1";
          }

          function cttrack_stop() {
            document.getElementById('cttrack_image').style.opacity = "0";
          }
          cttrack_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2301.10938">
          <papertitle>Compact Transformer Tracker with Correlative Masked Modeling</papertitle>
        </a>
        <br>
        <a href="https://github.com/SkyeSong38">Zikai Song</a>, 
        Run Luo,
        Junqing Yu<sup>&#134</sup>,
        <a href="http://homepage.cs.latrobe.edu.au/ypchen/">Yi-Ping Phoebe Chen</a>,
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>AAAI</em>, 2023 &nbsp;<font color="tomato"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://arxiv.org/abs/2301.10938">arXiv</a>
        /
        <a href="https://github.com/HUSTDML/CTTrack">Code</a>
        <p></p>
        <p>We demonstrate the basic vision transformer (ViT) architecture is sufficient for visual tracking with correlative masked modeling for information aggregation enhancement.</p>
      </td>
    </tr>

    <!-- Publication Section: DMU -->
    <tr onmouseout="dmu_stop()" onmouseover="dmu_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="dmu_image" style="opacity: 0;"><img src="./pub/dmu.png" width="140"></div>
          <img src="./pub/dmu.png" width="140">
        </div>
        <script type="text/javascript">
          function dmu_start() {
            document.getElementById('dmu_image').style.opacity = "1";
          }

          function dmu_stop() {
            document.getElementById('dmu_image').style.opacity = "0";
          }
          dmu_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/abs/2302.05160">
          <papertitle>Dual Memory Units with Uncertainty Regulation for Weakly Supervised Video Anomaly Detection</papertitle>
        </a>
        <br>
        <a href="https://github.com/henrryzh1">Hang Zhou</a>, 
        Junqing Yu<sup>&#134</sup>,
        <strong>Wei Yang</strong><sup>&#134</sup>
        <br>
        <em>AAAI</em>, 2023 &nbsp;<font color="tomato"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="https://arxiv.org/abs/2302.05160">arXiv</a>
        /
        <a href="https://github.com/henrryzh1/UR-DMU">Code</a>
        <p></p>
        <p>We propose an Uncertainty Regulated Dual Memory Units (UR-DMU) model to learn both the representations of normal data and discriminative features of abnormal data.</p>
      </td>
    </tr>

    <!-- Publication Section: HumanNeRF -->
    <tr onmouseout="humannerf_stop()" onmouseover="humannerf_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="humannerf_image" style="opacity: 0;"><video width="85%" height="85%" muted="" autoplay="" loop="">
            <source src="./pub/humannerf.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="humannerf_static_image" src="./pub/humannerf.png" width="160">
        </div>
        <script type="text/javascript">
          function humannerf_start() {
            document.getElementById('humannerf_image').style.opacity = "1";
            document.getElementById('humannerf_static_image').style.opacity = "0";
          }

          function humannerf_stop() {
            document.getElementById('humannerf_static_image').style.opacity = "1";
            document.getElementById('humannerf_image').style.opacity = "0";
          }
          humannerf_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/pdf/2112.02789.pdf">
          <papertitle>HumanNeRF: Efficiently Generated Human Radiance Field from Sparse Inputs</papertitle>
        </a>
        <br>
        <a href="https://zhaofuq.github.io/">Fuqiang Zhao</a>, 
        <strong>Wei Yang</strong>,
        <a href="https://jiakai-zhang.github.io/">Jiakai Zhang</a>,
        Pei Lin,
        Yingliang Zhang, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>, 
        <a href="http://xu-lan.com/">Lan Xu</a>
        <br>
        <em>CVPR</em>, 2022 &nbsp;
        <br>
        <a href="https://zhaofuq.github.io/humannerf/">Project Page</a>
        /        
        <a href="https://arxiv.org/pdf/2112.02789.pdf">arXiv</a>
        /
        <a href="https://github.com/zhaofuq/HumanNeRF">Code</a>
        /
        <a href="https://www.youtube.com/watch?v=CKeXnQiLCd4&ab_channel=ShanghaiTechDigitalHuman">Video</a>
        <p></p>
        <p>We present a neural representation with efficient generalization ability for high-fidelity free-view synthesis of dynamic humans.</p>
      </td>
    </tr>

    <!-- Publication Section: CSWinTT -->
    <tr onmouseout="cswintt_stop()" onmouseover="cswintt_start()">
          <td valign="middle"  width="25%">
            <div class="one">
              <div class="two" id="cswintt_image" style="opacity: 0;"><img src="./pub/cswintt.png" width="160"></div>
              <img src="./pub/cswintt.png" width="160">
            </div>
            <script type="text/javascript">
              function cswintt_start() {
                document.getElementById('cttrack_image').style.opacity = "1";
              }
    
              function cswintt_stop() {
                document.getElementById('cttrack_image').style.opacity = "0";
              }
              cswintt_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2205.03806">
              <papertitle>Transformer Tracking With Cyclic Shifting Window Attention</papertitle>
            </a>
            <br>
            <a href="https://github.com/SkyeSong38">Zikai Song</a>, 
            Junqing Yu,
            <a href="http://homepage.cs.latrobe.edu.au/ypchen/">Yi-Ping Phoebe Chen</a>,
            <strong>Wei Yang</strong>
            <br>
            <em>CVPR</em>, 2022 &nbsp;
            <br>
            <a href="https://arxiv.org/abs/2205.03806">arXiv</a>
            /
            <a href="https://github.com/SkyeSong38/CSWinTT">Code</a>
            <p></p>
            <p>CSWinTT is a new transformer architecture with multi-scale cyclic shifting window attention for visual object tracking, elevating the attention from pixel to window level.</p>
          </td>
        </tr>


    <!-- Publication Section: Artemis -->
    <tr onmouseout="artemis_stop()" onmouseover="artemis_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="artemis_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/artemis.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="artemis_static_image" src="./pub/artemis.png" width="160">
        </div>
        <script type="text/javascript">
          function artemis_start() {
            document.getElementById('artemis_static_image').style.opacity = "0";
            document.getElementById('artemis_image').style.opacity = "1";
          }

          function artemis_stop() {
            document.getElementById('artemis_image').style.opacity = "0";
            document.getElementById('artemis_static_image').style.opacity = "1";
          }
          artemis_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://haiminluo.github.io/publication/artemis/">
          <papertitle>Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</papertitle>
        </a>
        <br>
        <a href="https://haiminluo.github.io/">Haimin Luo</a>, 
        Teng Xu, 
        <a href="https://nowheretrix.github.io/">Yuheng Jiang</a>, 
        Chenglin Zhou, 
        Qiwei Qiu, 
        Yingliang Zhang, 
        <strong>Wei Yang</strong>,
        <a href="http://xu-lan.com/">Lan Xu</a>, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>SIGGRAPH</em>, 2022 &nbsp;
        <br>
        <a href="https://haiminluo.github.io/publication/artemis/">Project Page</a>
        /        
        <a href="https://arxiv.org/abs/2202.05628">arXiv</a>
        /
        <a href="https://github.com/HaiminLuo/Artemis">Code</a>
        /
        <a href="https://www.youtube.com/watch?v=Asbt3BYcZMM&ab_channel=ShanghaiTechDigitalHuman">Video</a>
        <p></p>
        <p>ARTEMIS, the core of which is a neural-generated (NGI) animal engine, enables interactive motion control, real-time animation and photo-realistic rendering of furry animals.</p>
      </td>
    </tr>

    <!-- Publication Section: NeuVV -->
    <tr onmouseout="neuvv_stop()" onmouseover="neuvv_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="neuvv_image" style="opacity: 0;"><img src="./pub/neuvv.png" width="160"></div>
          <img id="neuvv_static_image" src="./pub/neuvv.png" width="160">
        </div>
        <script type="text/javascript">
          function neuvv_start() {
            document.getElementById('neuvv_image').style.opacity = "1";
          }

          function neuvv_stop() {
            document.getElementById('neuvv_image').style.opacity = "0";
          }
          neuvv_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://arxiv.org/pdf/2202.06088.pdf">
          <papertitle>NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing</papertitle>
        </a>
        <br>
        <a href="https://jiakai-zhang.github.io/">Jiakai Zhang</a>,
        <a href="https://aoliao12138.github.io/">Liao Wang</a>, 
        <a href="http://xinhangliu.com/">Xinhang Liu</a>, 
        <a href="https://zhaofuq.github.io/">Fuqiang Zhao</a>, 
        Minzhang Li, 
        Haizhao Dai, 
        Boyuan Zhang, 
        <strong>Wei Yang</strong>,
        <a href="http://xu-lan.com/">Lan Xu</a>, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>arXiv preprint</em>, 2202.06088 &nbsp;
        <br>   
        <a href="https://arxiv.org/pdf/2202.06088.pdf">arXiv</a>
        <p></p>
        <p>NeuVV introduces a hyper-spherical harmonics (HH) decomposition for modeling smooth color variations over space and time.</p>
      </td>
    </tr>
    <!-- Publication Section: ACG-CVCNet -->
    <tr onmouseout="acgcvc_stop()" onmouseover="acgcvc_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="acgcvc_image" style="opacity: 0;"><img src="./pub/acgcvc.png" width="160"></div>
          <img id="acgcvc_static_image" src="./pub/acgcvc.png" width="160">
        </div>
        <script type="text/javascript">
          function acgcvc_start() {
            document.getElementById('acgcvc_image').style.opacity = "1";
          }

          function acgcvc_stop() {
            document.getElementById('acgcvc_image').style.opacity = "0";
          }
          acgcvc_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://dl.acm.org/doi/pdf/10.1145/3503161.3547926?casa_token=lbDaNtizQeUAAAAA:EGQEf4cDrdfqUY_uAckORK0Gny8NEUrNfFCTYMCPgG-cWfD_8mskpgb_FuJJQ__0_Nc9rLpvJJlgEQ">
          <papertitle>Self-Supervised Multi-view Stereo via Adjacent Geometry Guided Volume Completion</papertitle>
        </a>
        <br>
        Luoyuan Xu, 
        Tao Guan, 
        Yuesong Wang, 
        <a href="https://github.com/RoyalVane">Yawei Luo</a>
        Zhuo Chen, 
        Wenkai Liu, 
        <strong>Wei Yang</strong>
        <br>
        <em>ACM Multimedia</em>, 2022 &nbsp;
        <br> 
        <a href="https://dl.acm.org/doi/pdf/10.1145/3503161.3547926?casa_token=lbDaNtizQeUAAAAA:EGQEf4cDrdfqUY_uAckORK0Gny8NEUrNfFCTYMCPgG-cWfD_8mskpgb_FuJJQ__0_Nc9rLpvJJlgEQ">Paper</a>
        <p></p>
        <p>We propose the AGG-CVCNet to learn complete geometry inference from partial observations with high confidence.</p>
      </td>
    </tr>

    <!-- Publication Section: NPFA -->
    <tr onmouseout="npfa_stop()" onmouseover="npfa_start()">
          <td align="center" valign="middle"  width="25%">
            <div class="one">
              <div class="two_vc" id="npfa_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
                <source src="./pub/npfa.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              <img id="npfa_static_image" src="./pub/npfa.png" width="160">
            </div>
            <script type="text/javascript">
              function npfa_start() {
                document.getElementById('npfa_static_image').style.opacity = "0";
                document.getElementById('npfa_image').style.opacity = "1";
              }
    
              function npfa_stop() {
                document.getElementById('npfa_image').style.opacity = "0";
                document.getElementById('npfa_static_image').style.opacity = "1";
              }
              npfa_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/2202.05592">
              <papertitle>Video-driven Neural Physically-based Facial Asset for Production</papertitle>
            </a>
            <br>
            <a href="https://zhanglongwen.com/">Longwen Zhang</a>, 
            Chuxiao Zeng, 
            Qixuan Zhang, 
            Hongyang Lin, 
            <a href="https://www.caoruixiang.com/resume/Resume.pdf">Ruixiang Cao</a>, 
            <strong>Wei Yang</strong>,
            <a href="http://xu-lan.com/">Lan Xu</a>, 
            <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
            <br>
            <em>SIGGRAPH Asia</em>, 2022 &nbsp;
            <br>
            <a href="https://sites.google.com/view/npfa/">Project Page</a>
            /
            <a href="https://arxiv.org/abs/2202.05592">arXiv</a>
            /
            <a href="https://www.youtube.com/watch?v=BifBssghYWQ&ab_channel=ShanghaiTechDigitalHuman">Video</a>
            <p></p>
            <p>We present a learning-based, video-driven approach for generating dynamic facial geometries with high-quality physically-based assets</p>
          </td>
    </tr>

    <!-- Publication Section: Tightcap -->
    <tr onmouseout="tightcap_stop()" onmouseover="tightcap_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="tightcap_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/tightcap.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
          <img id="tightcap_static_image" src="./pub/tightcap.jpg" width="160">
        </div>
        <script type="text/javascript">
          function tightcap_start() {
            document.getElementById('tightcap_static_image').style.opacity = "0";
            document.getElementById('tightcap_image').style.opacity = "1";
          }

          function tightcap_stop() {
            document.getElementById('tightcap_image').style.opacity = "0";
            document.getElementById('tightcap_static_image').style.opacity = "1";
          }
          tightcap_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://github.com/ChenFengYe/TightCap">
          <papertitle>Tightcap: 3D human shape capture with clothing tightness field</papertitle>
        </a>
        <br>
        <a href="https://chenxin.tech/">Xin Chen</a>, 
        Anqi Pang, 
        <strong>Wei Yang</strong>,
        <a href="https://peihaowang.github.io/">Peihao Wang</a>,
        <a href="http://xu-lan.com/">Lan Xu</a>, 
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>SIGGRAPH</em>, 2022 (TOG)&nbsp;
        <br>
        <a href="https://chenxin.tech/TightCap/">Project Page</a>
        /
        <a href="https://arxiv.org/pdf/1904.02601.pdf">arXiv</a>
        /
        <a href="https://github.com/ChenFengYe/TightCap">Code</a>
        /
        <a href="https://www.youtube.com/watch?v=2_Vlq34s80Y&ab_channel=XinChen">Video</a>
        <p></p>
        <p>TightCap is a data-driven scheme to capture both the human shape and dressed garments accurately with only a single 3D human scan.</p>
      </td>
    </tr>

    <!-- Publication Section: SportsCap -->
    <tr onmouseout="sportscap_stop()" onmouseover="sportscap_start()">
          <td align="center" valign="middle"  width="25%">
            <div class="one">
              <div class="two_vc" id="sportscap_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
                <source src="./pub/sportscap.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              <img id="sportscap_static_image" src="./pub/sportscap.png" width="160">
            </div>
            <script type="text/javascript">
              function sportscap_start() {
                document.getElementById('sportscap_static_image').style.opacity = "0";
                document.getElementById('sportscap_image').style.opacity = "1";
              }
    
              function sportscap_stop() {
                document.getElementById('sportscap_image').style.opacity = "0";
                document.getElementById('sportscap_static_image').style.opacity = "1";
              }
              sportscap_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://chenxin.tech/files/Paper/IJCV2020_Sport/project_page_SportsCap/index.htm">
              <papertitle>SportsCap: Monocular 3D Human Motion Capture and Fine-grained Understanding in Challenging Sports Videos</papertitle>
            </a>
            <br>
            <a href="https://chenxin.tech/">Xin Chen</a>, 
            Anqi Pang, 
            <strong>Wei Yang</strong>,
            <a href="https://yuexinma.me/">Yuexin Ma</a>, 
            <a href="http://xu-lan.com/">Lan Xu</a>, 
            <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
            <br>
            <em>IJCV</em>, 2022 (TOG)&nbsp;
            <br>
            <a href="https://chenxin.tech/files/Paper/IJCV2020_Sport/project_page_SportsCap/index.htm">Project Page</a>
            /
            <a href="https://arxiv.org/abs/2104.11452">arXiv</a>
            /
            <a href="https://github.com/ChenFengYe/SportsCap">Code</a>
            /
            <a href="https://www.youtube.com/watch?v=TrqAaaX97KY&ab_channel=ShanghaiTechDigitalHuman">Video</a>
            <p></p>
            <p>SportsCap -- the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input</p>
          </td>
      </tr>

    <!-- Publication Section: XSfM -->
    <tr onmouseout="xsfm_stop()" onmouseover="xsfm_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="xsfm_image" style="opacity: 0;"><img src="./pub/xsfm.png" width="160"></div>
          <img id="xsfm_static_image" src="./pub/xsfm.png" width="160">
        </div>
        <script type="text/javascript">
          function xsfm_start() {
            document.getElementById('xsfm_image').style.opacity = "1";
          }

          function xsfm_stop() {
            document.getElementById('xsfm_image').style.opacity = "0";
          }
          xsfm_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8918483">
          <papertitle>Structure From Motion on XSlit Cameras</papertitle>
        </a>
        <br>
        <strong>Wei Yang</strong>,
        Yingliang Zhang,
        <a href="https://cs.gmu.edu/~jinweiye/">Jinwei Ye</a>,
        <a href="https://yeauxji.github.io/">Yu Ji</a>,
        <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
        Mingyuan Zhou,
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>TPAMI</em>, 2019 &nbsp;
        <br> 
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8918483">Paper</a>
        <p></p>
        <p>We present a structure-from-motion (SfM) framework based on a special type of multi-perspective camera called the cross-slit or XSlit camera.</p>
      </td>
    </tr>

    <!-- Publication Section: dhuman -->
    <tr onmouseout="dhuman_stop()" onmouseover="dhuman_start()">
          <td align="center" valign="middle"  width="25%">
            <div class="one">
              <div class="two_vc" id="dhuman_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
                <source src="./pub/dhuman.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>              
                <img id="dhuman_static_image" src="./pub/dhuman.png" width="160">
            </div>
            <script type="text/javascript">
              function dhuman_start() {
                document.getElementById('dhuman_static_image').style.opacity = "0";
                document.getElementById('dhuman_image').style.opacity = "1";
              }
    
              function dhuman_stop() {
                document.getElementById('dhuman_image').style.opacity = "0";
                document.getElementById('dhuman_static_image').style.opacity = "1";
              }
              dhuman_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://arxiv.org/abs/1801.10434">
              <papertitle>Robust 3D Human Motion Reconstruction via Dynamic Template Construction</papertitle>
            </a>
            <br>
            <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
            <a href="https://yeauxji.github.io/">Yu Ji</a>,
            <strong>Wei Yang</strong>,
            <a href="https://cs.gmu.edu/~jinweiye/">Jinwei Ye</a>,
            <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
            <br>
            <em>3DV</em>, 2017 &nbsp;<font color="tomato"><strong>(Spotlight Oral Presentation)</strong></font>
            <br> 
            <a href="https://arxiv.org/abs/1801.10434">arXiv</a>
            /
            <a href="https://www.youtube.com/watch?v=nYXG7kxvM3c&ab_channel=ZhongLi">Video</a>
            /
            <a href="https://vic.shanghaitech.edu.cn/vrvc/en/Code.html">Data</a>
            <p></p>
            <p>We generate a global full-body template by registering all poses in the acquired motion sequence, and then construct a deformable graph by utilizing the rigid components in the global template.</p>
          </td>
        </tr>

    <!-- Publication Section: LF Scanner -->
    <tr onmouseout="lfscanner_stop()" onmouseover="lfscanner_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="lfscanner_image" style="opacity: 0;"><img src="./pub/lfscanner.png" width="150"></div>
          <img id="lfscanner_static_image" src="./pub/lfscanner.png" width="150">
        </div>
        <script type="text/javascript">
          function lfscanner_start() {
            document.getElementById('lfscanner_image').style.opacity = "1";
          }

          function lfscanner_stop() {
            document.getElementById('lfscanner_image').style.opacity = "0";
          }
          lfscanner_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://ieeexplore.ieee.org/document/7951484">
          <papertitle>The Light Field 3D Scanner</papertitle>
        </a>
        <br>
        Yingliang Zhang,
        <a href="https://sites.google.com/site/lizhong19900216">Zhong Li</a>,
        <strong>Wei Yang</strong>,
        <a href="https://peihongyu.com/">Peihong Yu</a>,
        Haiting Lin,
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>ICCP</em>, 2017 &nbsp;
        <br> 
        <a href="https://ieeexplore.ieee.org/document/7951484">Paper</a>
        <p></p>
        <p>We use the light field (LF) camera such as Lytro and Raytrix as a virtual 3D scanner.</p>
      </td>
    </tr>

    <!-- Publication Section: rayspace -->
    <tr onmouseout="rayspace_stop()" onmouseover="rayspace_start()">
          <td align="center" valign="middle"  width="25%">
            <div class="one">
              <div class="two" id="rayspace_image" style="opacity: 0;"><img src="./pub/rayspace.png" width="150"></div>
              <img id="rayspace_static_image" src="./pub/rayspace.png" width="150">
            </div>
            <script type="text/javascript">
              function rayspace_start() {
                document.getElementById('rayspace_image').style.opacity = "1";
              }
    
              function rayspace_stop() {
                document.getElementById('rayspace_image').style.opacity = "0";
              }
              rayspace_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Ray_Space_Features_ICCV_2017_paper.pdf">
              <papertitle>Ray Space Features for Plenoptic Structure-from-Motion</papertitle>
            </a>
            <br>
            Yingliang Zhang,
            <a href="https://peihongyu.com/">Peihong Yu</a>,
            <strong>Wei Yang</strong>,
            Yuanxi Ma,
            <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
            <br>
            <em>ICCV</em>, 2017 &nbsp;
            <br> 
            <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Ray_Space_Features_ICCV_2017_paper.pdf">Paper</a>
            <p></p>
            <p>We present a comprehensive theory on ray geometry transforms under light field pose variations, and derive the transforms of three typical ray manifolds.</p>
          </td>
        </tr>

    <!-- Publication Section: xara -->
    <tr onmouseout="xara_stop()" onmouseover="xara_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="xara_image" style="opacity: 0;"><img src="./pub/xara.png" width="150"></div>
          <img id="xara_static_image" src="./pub/xara.png" width="150">
        </div>
        <script type="text/javascript">
          function xara_start() {
            document.getElementById('xara_image').style.opacity = "1";
          }

          function xara_stop() {
            document.getElementById('xara_image').style.opacity = "0";
          }
          xara_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Resolving_Scale_Ambiguity_ICCV_2015_paper.pdf">
          <papertitle>Resolving Scale Ambiguity via XSlit Aspect Ratio Analysis</papertitle>
        </a>
        <br>
        <strong>Wei Yang</strong>,
        Haiting Lin,
        <a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>ICCV</em>, 2015 &nbsp;
        <br> 
        <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Yang_Resolving_Scale_Ambiguity_ICCV_2015_paper.pdf">Paper</a>
        <p></p>
        <p> We present the depth dependent aspecratio (DDAR) property that can be used to 3D recovery.</p>
      </td>
    </tr>
    
    <!-- Publication Section: AO -->
    <tr onmouseout="ao_stop()" onmouseover="ao_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two_vc" id="ao_image" style="opacity: 0;"><video width="100%" height="100%" muted="" autoplay="" loop="">
            <source src="./pub/ao.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>              
            <img id="ao_static_image" src="./pub/ao.png" width="140">
        </div>
        <script type="text/javascript">
          function ao_start() {
            document.getElementById('ao_static_image').style.opacity = "0";
            document.getElementById('ao_image').style.opacity = "1";
          }

          function ao_stop() {
            document.getElementById('ao_image').style.opacity = "0";
            document.getElementById('ao_static_image').style.opacity = "1";
          }
          ao_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Yang_Ambient_Occlusion_via_2015_CVPR_paper.pdf">
          <papertitle>Ambient Occlusion via Compressive Visibility Estimation</papertitle>
        </a>
        <br>
        <strong>Wei Yang</strong>,
        <a href="https://yeauxji.github.io/">Yu Ji</a>,
        Haiting Lin,
        Yang Yang,
        <a href="http://www.singbingkang.com/">Sing Bing Kang</a>,
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>CVPR</em>, 2015 &nbsp;
        <br> 
        <a href="https://openaccess.thecvf.com/content_cvpr_2015/papers/Yang_Ambient_Occlusion_via_2015_CVPR_paper.pdf">Paper</a>
        <p></p>
        <p>We present a novel computational imaging solution for recovering AO by adopting a compressive sensing framework.</p>
      </td>
    </tr>

    <!-- Publication Section: xdof -->
    <tr onmouseout="xdof_stop()" onmouseover="xdof_start()">
          <td align="center" valign="middle"  width="25%">
            <div class="one">
              <div class="two" id="xdof_image" style="opacity: 0;"><img src="./pub/xdof.png" width="160"></div>
              <img id="xdof_static_image" src="./pub/xdof.png" width="160">
            </div>
            <script type="text/javascript">
              function xdof_start() {
                document.getElementById('xdof_image').style.opacity = "1";
              }
    
              function xdof_stop() {
                document.getElementById('xdof_image').style.opacity = "0";
              }
              xdof_stop()
            </script>
          </td>
          <td valign="middle" width="75%">
            <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ECCV-2014/papers/8691/86910753.pdf">
              <papertitle>Depth-of-field and Coded Aperture Imaging on Xslit Lens</papertitle>
            </a>
            <br>
            <a href="https://cs.gmu.edu/~jinweiye/">Jinwei Ye</a>,
            <a href="https://yeauxji.github.io/">Yu Ji</a>,
            <strong>Wei Yang</strong>,
            <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
            <br>
            <em>ECCV</em>, 2014 &nbsp;<font color="tomato"><strong>(Oral Presentation)</strong></font>
            <br> 
            <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ECCV-2014/papers/8691/86910753.pdf">Paper</a>
            /
            <a href="eccv2014_yu_coded_aperture">Video</a>
            <p></p>
            <p>We explore coded aperture solutions on a special non-centric lens called the crossedslit (XSlit) lens.</p>
          </td>
      </tr>
    <!-- Publication Section: ccp -->
    <tr onmouseout="ccp_stop()" onmouseover="ccp_start()">
      <td align="center" valign="middle"  width="25%">
        <div class="one">
          <div class="two" id="ccp_image" style="opacity: 0;"><img src="./pub/ccp.png" width="160"></div>
          <img id="ccp_static_image" src="./pub/ccp.png" width="160">
        </div>
        <script type="text/javascript">
          function ccp_start() {
            document.getElementById('ccp_image').style.opacity = "1";
          }

          function ccp_stop() {
            document.getElementById('ccp_image').style.opacity = "0";
          }
          ccp_stop()
        </script>
      </td>
      <td valign="middle" width="75%">
        <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ECCV-2014/papers/8689/86890220.pdf">
          <papertitle>Coplanar Common Points in Non-centric Cameras</papertitle>
        </a>
        <br>
        <strong>Wei Yang</strong>,
        <a href="https://yeauxji.github.io/">Yu Ji</a>,
        <a href="https://cs.gmu.edu/~jinweiye/">Jinwei Ye</a>,
        S. Susan Young,
        <a href="http://www.yu-jingyi.com/">Jingyi Yu</a>
        <br>
        <em>ECCV</em>, 2014 &nbsp;
        <br> 
        <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ECCV-2014/papers/8689/86890220.pdf">Paper</a>
        <p></p>
        <p>We address the problem of determining CCP existence in general non-centric cameras.</p>
      </td>
  </tr>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          I borrowed this website template from <a href="https://jonbarron.info/">Jon Barron</a>, thanks! <br>
	    </font>
        </p>
        </td>
      </tr>
      </tbody></table>

    </td>
    </tr>
  </tbody></table>
  


</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>